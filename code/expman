#!/usr/bin/env python3

import itertools
import rich
import typer
import os
from datetime import datetime, timedelta
import regex as re

from typing import Tuple

from rich.console import Console
from rich.markdown import Markdown
from rich.padding import Padding
from rich.columns import Columns
from rich.panel import Panel
from rich.table import Table
from rich.progress import track
from rich.progress import Progress, TaskID

import glob
from itertools import groupby
import wandb

from wandb.apis.public import File

from exputils import *


@app.command("results")
def results_stats():
    """
    Plot the results of the runs
    """
    api:wandb.Api = wandb.Api()
    entity:str
    project:str 

    entity, project = "mlgroup", "FederatedAdaboost "
    runs = list(api.runs(entity + "/" + project, filters={"tags": "IJCNN"}))

    console = Console(record=True)
    runs = sorted(runs, key=lambda x: x.config["non_iidness"] + x.config["dataset"] + x.config["model"])

    exps_by_skw = groupby(
        runs, lambda r: r.config["non_iidness"] + "|" + r.config["dataset"])

    ranks = {"FedAlgorithms.adaboost": [], "FedAlgorithms.distsamme": [], "FedAlgorithms.samme": [], "FedAlgorithms.preweaksamme": []}
    ranks_by_skw = {
        "Noniidness.uniform": {"FedAlgorithms.adaboost": [], "FedAlgorithms.distsamme": [], "FedAlgorithms.samme": [], "FedAlgorithms.preweaksamme": []},
        "Noniidness.num_examples_skw": {"FedAlgorithms.adaboost": [], "FedAlgorithms.distsamme": [], "FedAlgorithms.samme": [], "FedAlgorithms.preweaksamme": []},
        "Noniidness.lbl_skw": {"FedAlgorithms.adaboost": [], "FedAlgorithms.distsamme": [], "FedAlgorithms.samme": [], "FedAlgorithms.preweaksamme": []},
        "Noniidness.dirichlet_lbl_skw": {"FedAlgorithms.adaboost": [], "FedAlgorithms.distsamme": [], "FedAlgorithms.samme": [], "FedAlgorithms.preweaksamme": []},
        "Noniidness.pathological_skw": {"FedAlgorithms.adaboost": [], "FedAlgorithms.distsamme": [], "FedAlgorithms.samme": [], "FedAlgorithms.preweaksamme": []},
        "Noniidness.covariate_shift": {"FedAlgorithms.adaboost": [], "FedAlgorithms.distsamme": [], "FedAlgorithms.samme": [], "FedAlgorithms.preweaksamme": []}
    }

    for non_iidness_dataset, exps in exps_by_skw:
        non_iidness, dataset = non_iidness_dataset.split("|")

        table = Table(title=f"skw:{non_iidness} dataset:{dataset}")
        table.add_column("rank", justify="right", style="bold white")
        table.add_column("method", justify="right", style="magenta")
        table.add_column("f1", justify="right", style="green")

        exps = list(exps)
        model_groups = groupby(exps, lambda r: r.config["model"])
        model_avg = []

        try:
            for model, runs in model_groups:
                runs = list(runs)
                f1s = [r.summary["test"]["f1"] for r in runs]
                f1_avg = sum(f1s) / len(f1s)
                model_avg.append((model, f1_avg))
        except KeyError as e:
            console.print(f"Error in retrieving key {e} for dataset:{dataset} skw:{non_iidness} model:{model}", style="bold yellow")
            continue


        for index, exp_f1 in enumerate(sorted(model_avg, key=lambda x:x[1], reverse=True)):
            model, f1 = exp_f1
            try:
                ranks[model].append(index+1)
                ranks_by_skw[non_iidness][model].append(index+1)

                if model == "FedAlgorithms.adaboost":
                    model = f"[bold]{model}[/]"

                table.add_row(str(index), model, str(f1))
            except KeyError as exception:
                console.print(f"Error with experiment -- {exception}")

        console.print(table)

    ranks_table = Table(title="Ranks")
    ranks_table.add_column("method", justify="left", style="magenta")
    ranks_table.add_column("avg rank", justify="right", style="green")

    for key in ranks.keys():
        rank_list = ranks[key]
        ranks_table.add_row(key, str(sum(rank_list) / len(rank_list)))

    console.print(ranks_table)


    for key in ranks_by_skw.keys():
        rbs_table = Table(title=f"Ranks for skw {key}")
        rbs_table.add_column("method", justify="left", style="magenta")
        rbs_table.add_column("avg rank", justify="left", style="green")
        skw_ranks = ranks_by_skw[key]

        for model in skw_ranks.keys():
            rank_list = skw_ranks[model]
            rbs_table.add_row(model, str(sum(rank_list) / len(rank_list)))

        console.print(rbs_table)


    console.save_html("results.html")





def generate_exp_targets(test_run_opt: str, f: File, verbose=False) -> None:
    """
    Generates the Makefile targets for the experiments that needs to be done.
    """
    experiments: list[ExpDescription]
    experiments, _ = experiment_list(verbose)

    experiment_tags: list[str] = []
    
    for experiment in track(experiments, "Generating experiment targets"):
        ds: str
        seed: int
        model: str
        noniid: str
        ds, seed, model, noniid = experiment

        experiment_tags.append(
            f"logs/ijcnnexps_ds_{ds}_model_{model}_noniid_{noniid}_seed_{seed}.log")
        print(f"{experiment_tags[-1]}:", file=f)
        print(
            f"\tpython3 ijcnn_exps.py --seed={seed} --n-clients=10 --model={model} --non-iidness={noniid} --tags=IJCNN {test_run_opt} {ds}", file=f)

    exp_tags_str: str = " ".join(experiment_tags)
    print(f"all:{exp_tags_str}", file=f)

    print(f"clean_all_logs:", file=f)
    print(f"\trm -f logs/*.log", file=f)
    print(f"\trm -f logs/*.err", file=f)

def generate_plot_targets(f:File, verbose=False) -> None:
    """
    Generates the Makefile targets for the plots that needs to be done.
    """
    pl = plotlist(verbose)
    filenames = []

    for ds, noniidness in track(pl, "Generating plot targets"):
        fname = plot_fname(ds, noniidness)
        print(f"{fname}:", file=f)
        print(f"\tpython3 mkgraphs.py plot {ds} {noniidness}", file=f)

        filenames.append(fname)

    filename_str = " ".join(filenames)
    print(f"all_plots:{filename_str}", file=f)
    print(f"clean_all_plots:", file=f)
    print(f"\trm -f images/*.pdf", file=f)
    

def launched_exps_stats() -> dict[str,int]:
    """
    Returns statistics about the experiments that have been launched. 
    The statistics are returned as a dictionary with the following keys:
        "completed": the number of experiments that have been launched and completed
        "failed": the number of experiments that have been launched and failed
        "running": the number of experiments that have been launched and are still running
    """
    completed: list[str] = glob.glob("logs/*.log")
    failed: list[str] = glob.glob("logs/*.err")
    running: list[str] = glob.glob("logs/*.run")

    return { "completed": len(completed), "failed": len(failed), "running": len(running) }

def get_elapsed(exp:str) -> timedelta:
    """
    Returns the elapsed time for the experiment exp. 
    Exp must be a running experiment.
    """
    start_time:datetime = datetime.fromtimestamp(os.path.getmtime(exp))
    curr_time:datetime = datetime.now()

    delta:timedelta = curr_time - start_time
    return delta


def print_general_stats(experiments: list[ExpDescription], skipped: list[ExpDescription], launched_exps: dict[str, int]) -> None:
    """
    Prints general statistics about the experiments that have been launched.
    Statistics include:
        - the number of experiments that have been generated
        - the number of experiments that have been skipped
        - the total number of experiments (generated + skipped)

        - the number of experiments that completed
        - the number of experiments that failed
        - the number of experiments that are still running
        - the total number of experiments that have been launched
    """
    
    total_exps_md: str = f"""
# Total experiments stats 
- Generated: {len(experiments)}
- Skipped: {len(skipped)}
- **Total**: {len(experiments) + len(skipped)}
"""
    launched_exps_md: str = f"""
# Launched experiments stats
- *Running*: {launched_exps["running"]}    
- Completed: {launched_exps["completed"]}
- **Failed**: {launched_exps["failed"]}
- **Total**: {launched_exps["completed"] + launched_exps["failed"] + launched_exps["running"]}
"""

    panel1: Panel = Panel.fit(Markdown(total_exps_md), width=40)
    panel2: Panel = Panel.fit(Markdown(launched_exps_md), width=40)

    console.print(Columns([panel1, panel2]))


def print_failed_exps_table(launched_exps: dict[str, int]) -> None:
    """
    Prints a table with the experiments that failed.
    """
    if launched_exps["failed"] > 0:
        failed_exps: list[str] = glob.glob("logs/*.err")
        table: Table = Table(title="[bold red]Failed experiments[/]")
        table.add_column("#", justify="right", style="bold")
        table.add_column("Experiment", justify="left", style="magenta")
        table.add_column("WANDB name", justify="right", style="green")

        for i, exp in enumerate(failed_exps):
            table.add_row(str(i), exp, log_name_to_wandb(exp))

        console.print(table)

# Use a regex to parse log names in the format "logs/ijcnnexps_ds_<ds>_model_<model>_noniid_<noniid>_seed_<seed>.log"
# to extract the dataset, model, noniid and seed and return them in the format "<ds>_<model>_<noniid>_<seed>"
def log_name_to_wandb(logname:str) -> str:
    """
    Returns the name of the experiment in wandb format.
    logname: the name of the experiment in log format.
    """
    regexp = re.compile(r"logs/ijcnnexps_ds_(?P<ds>\w+)_model_(?P<model>(\w|\.)+)_noniid_(?P<noniid>\w+)_seed_(?P<seed>\d+).(run|log|err)")
    match = regexp.match(logname)
    if match is None:
        raise ValueError(f"The log name {logname} does not match the expected format")
    
    ds:str = match.group("ds")
    model:str = match.group("model")
    noniid:str = match.group("noniid")
    seed:int = int(match.group("seed"))

    return f"{ds}_{model}_{noniid}_{seed}"


def print_running_exps_table(launched_exps: dict[str, int]) -> None:
    """
    Prints a table with the experiments that are still running and their current running time.
    """
    if launched_exps["running"] > 0:
        running_exps: list[str] = glob.glob("logs/*.run")
        running_times: list[timedelta] = list(
            map(lambda exp: get_elapsed(exp), running_exps))

        running_info = list(zip(running_exps, running_times))
        running_info = sorted(running_info, key=lambda x: x[1])

        table: Table = Table(title="[bold green]Running experiments[/]")
        table.add_column("#", justify="right", style="bold")
        table.add_column("Experiment", justify="left", style="magenta")
        table.add_column("WANDB name", justify="right", style="green")
        table.add_column("Elapsed time", justify="right", style="yellow")

        for i, exp_info in enumerate(running_info):
            delta_time: timedelta = exp_info[1]
            hours: int = delta_time.seconds // 3600
            minutes: int = (delta_time.seconds % 3600) // 60
            seconds: int = delta_time.seconds % 60

            table.add_row(str(i), exp_info[0], log_name_to_wandb(exp_info[0]),
                          f"{hours}h {minutes}m {seconds}s")

        console.print(table)


def print_progress_bar(experiments: list[ExpDescription], launched_exps: dict[str, int]) -> None:
    """
    Prints a progress bar based on the number of experiments that have completed (w.r.t., the total number of
    experiments generated and not skipped).
    """
    with Progress() as progress:

        task1: TaskID = progress.add_task(
            "[red]Exp progress:", total=len(experiments))

        progress.update(task1, advance=launched_exps["completed"])


@app.command()
def stats(verbose:bool=False) -> None:
    """
    Prints statistics about the experiments to be generated.

    If verbose is True, then the list of skipped experiments is printed to stdout.
    """    
    experiments:list[ExpDescription]
    skipped:list[ExpDescription]

    experiments, skipped = experiment_list(verbose)
    launched_exps:dict[str,int] = launched_exps_stats()

    print_general_stats(experiments, skipped, launched_exps)
    print_failed_exps_table(launched_exps)
    print_running_exps_table(launched_exps)
    print_progress_bar(experiments, launched_exps)



@app.command()
def generate_makefiles(outfile:str=typer.Argument("Makefile"),
                        exps:bool=typer.Option(True, help="Generate entries for the experiments"),
                        plots:bool=typer.Option(True, help="Generate entries for the plots"),
                        verbose:bool=False, test_run:bool=True) -> None:
    """
    Generates a Makefile allowing to launch the experiments presented in  (Polato, Esposito, et al. 2022)

    If verbose is True, then the list of skipped experiments is printed to stdout.
    If test_run is True, then the Makefile is generated for a test run (i.e., the experiments will be
    configured to *not* connect to wandb and the number of weak learners will be set to 1).
    """

    test_run_opt:str = "--test-run" if test_run else "--no-test-run"

    with open(outfile, "w") as f:
        if exps:
            generate_exp_targets(test_run_opt, f, verbose=verbose)
        
        if plots:
            generate_plot_targets(f, verbose=verbose)


if __name__ == '__main__':
    app()
