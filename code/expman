#!/usr/bin/env python3

import itertools
import rich
import typer
import os
from datetime import datetime, timedelta
import time

from typing import Tuple

from rich.console import Console
from rich.markdown import Markdown
from rich.padding import Padding
from rich.columns import Columns
from rich.panel import Panel
from rich.table import Table
from rich.progress import Progress, TaskID
import glob

ExpDescription = Tuple[str, int, str, str]

console = Console()
app = typer.Typer()

EXPS = "adult,letter,forestcover,splice,vehicle,vowel,segmentation,kr-vs-kp,sat,pendigits".split(",")
SEEDS = [0, 1, 2, 3]
MODELS = "samme,distsamme,preweaksamme,adaboost.f1".split(",")
NONIID = "uniform,num_examples_skw,lbl_skw,dirichlet_lbl_skw,pathological_skw,covariate_shift".split(",")

def experiment_to_skip(ds:str, seed:int, model:str, noniid:str, verbose:bool):
    """
    Returns True if the experiment should be skipped.
    Presently: datasets "adult" e "kr-vs-kp" should be skipped when iidness is in 
        "lbl_skw", "dirichlet_lbl_skw", "pathological_skw" (these are binary datasets which cannot
        work with these types of non-iidness)
    In addition: if noniid is "pathological_skw", then we are avoiding the experiments since there is
        a problem in how the data are split in some cases.
    """
    if ds in ["adult", "kr-vs-kp"] and noniid in ["lbl_skw", "dirichlet_lbl_skw", "pathological_skw"]:
        if verbose:
            console.log(f"[bold yellow]Skipping[/] {ds} {seed} {model} {noniid}")

        return True

    if noniid == "pathological_skw":
        if verbose:
            console.log(f"[bold yellow]Skipping[/] {ds} {seed} {model} {noniid}")
        return True

    return False

def experiment_list(verbose:bool=False) -> Tuple[list[ExpDescription], list[ExpDescription]]:
    """
    Returns a list of all the experiments that should be launched (i.e., the 
    list of all experiments minus the ones that should be skipped).

    if verbose is True, then skipped experiments are logged to stdout.
    """
    experiments:list[ExpDescription] = list(itertools.product(EXPS, SEEDS, MODELS, NONIID))
    result = []
    skipped = []

    for experiment in experiments:
        ds:str
        seed:int
        model:str
        noniid:str 
        
        ds, seed, model, noniid = experiment

        if experiment_to_skip(ds, seed, model, noniid, verbose):
            skipped.append(experiment)
        else:        
            result.append(experiment)

    return result, skipped

def launched_exps_stats() -> dict[str,int]:
    """
    Returns statistics about the experiments that have been launched. 
    The statistics are returned as a dictionary with the following keys:
        "completed": the number of experiments that have been launched and completed
        "failed": the number of experiments that have been launched and failed
        "running": the number of experiments that have been launched and are still running
    """
    completed: list[str] = glob.glob("logs/*.log")
    failed: list[str] = glob.glob("logs/*.err")
    running: list[str] = glob.glob("logs/*.run")

    return { "completed": len(completed), "failed": len(failed), "running": len(running) }

def get_elapsed(exp:str) -> timedelta:
    """
    Returns the elapsed time for the experiment exp. 
    Exp must be a running experiment.
    """
    start_time:datetime = datetime.fromtimestamp(os.path.getmtime(exp))
    curr_time:datetime = datetime.now()

    delta:timedelta = curr_time - start_time
    return delta


def print_general_stats(experiments: list[ExpDescription], skipped: list[ExpDescription], launched_exps: dict[str, int]) -> None:
    """
    Prints general statistics about the experiments that have been launched.
    Statistics include:
        - the number of experiments that have been generated
        - the number of experiments that have been skipped
        - the total number of experiments (generated + skipped)

        - the number of experiments that completed
        - the number of experiments that failed
        - the number of experiments that are still running
        - the total number of experiments that have been launched
    """
    
    total_exps_md: str = f"""
# Total experiments stats 
- Generated: {len(experiments)}
- Skipped: {len(skipped)}
- **Total**: {len(experiments) + len(skipped)}
"""
    launched_exps_md: str = f"""
# Launched experiments stats
- *Running*: {launched_exps["running"]}    
- Completed: {launched_exps["completed"]}
- **Failed**: {launched_exps["failed"]}
- **Total**: {launched_exps["completed"] + launched_exps["failed"] + launched_exps["running"]}
"""

    panel1: Panel = Panel.fit(Markdown(total_exps_md), width=40)
    panel2: Panel = Panel.fit(Markdown(launched_exps_md), width=40)

    console.print(Columns([panel1, panel2]))


def print_failed_exps_table(launched_exps: dict[str, int]) -> None:
    """
    Prints a table with the experiments that failed.
    """
    if launched_exps["failed"] > 0:
        failed_exps: list[str] = glob.glob("logs/*.err")
        table: Table = Table(title="[bold red]Failed experiments[/]")
        table.add_column("#", justify="right", style="bold")
        table.add_column("Experiment", justify="left", style="magenta")
        for i, exp in enumerate(failed_exps):
            table.add_row(str(i), exp)

        console.print(table)


def print_running_exps_table(launched_exps: dict[str, int]) -> None:
    """
    Prints a table with the experiments that are still running and their current running time.
    """
    if launched_exps["running"] > 0:
        running_exps: list[str] = glob.glob("logs/*.run")
        running_times: list[timedelta] = list(
            map(lambda exp: get_elapsed(exp), running_exps))

        running_info = list(zip(running_exps, running_times))
        running_info = sorted(running_info, key=lambda x: x[1])

        table: Table = Table(title="[bold green]Running experiments[/]")
        table.add_column("#", justify="right", style="bold")
        table.add_column("Experiment", justify="left", style="magenta")
        table.add_column("Elapsed time", justify="right", style="yellow")

        for i, exp_info in enumerate(running_info):
            delta_time: timedelta = exp_info[1]
            hours: int = delta_time.seconds // 3600
            minutes: int = (delta_time.seconds % 3600) // 60
            seconds: int = delta_time.seconds % 60

            table.add_row(str(i), exp_info[0],
                          f"{hours}h {minutes}m {seconds}s")

        console.print(table)


def print_progress_bar(experiments: list[ExpDescription], launched_exps: dict[str, int]) -> None:
    """
    Prints a progress bar based on the number of experiments that have completed (w.r.t., the total number of
    experiments generated and not skipped).
    """
    with Progress() as progress:

        task1: TaskID = progress.add_task(
            "[red]Exp progress:", total=len(experiments))

        progress.update(task1, advance=launched_exps["completed"])


@app.command()
def stats(verbose:bool=False) -> None:
    """
    Prints statistics about the experiments to be generated.

    If verbose is True, then the list of skipped experiments is printed to stdout.
    """    
    experiments:list[ExpDescription]
    skipped:list[ExpDescription]

    experiments, skipped = experiment_list(verbose)
    launched_exps:dict[str,int] = launched_exps_stats()

    print_general_stats(experiments, skipped, launched_exps)
    print_failed_exps_table(launched_exps)
    print_running_exps_table(launched_exps)
    print_progress_bar(experiments, launched_exps)
    

    
@app.command()
def generate_makefiles(outfile:str=typer.Argument("Makefile"), verbose:bool=False, test_run:bool=True) -> None:
    """
    Generates a Makefile allowing to launch the experiments presented in  (Polato, Esposito, et al. 2022)

    If verbose is True, then the list of skipped experiments is printed to stdout.
    If test_run is True, then the Makefile is generated for a test run (i.e., the experiments will be
    configured to *not* connect to wandb and the number of weak learners will be set to 1).
    """

    test_run_opt:str = "--test-run" if test_run else "--no-test-run"

    experiments:list[ExpDescription]
    experiments, _ = experiment_list()

    with open(outfile, "w") as f:
        experiment_tags:list[str] = []
        for experiment in experiments:
            ds:str
            seed:int
            model:str
            noniid:str
            ds, seed, model, noniid = experiment

            experiment_tags.append(f"logs/ijcnnexps_ds_{ds}_model_{model}_noniid_{noniid}_seed_{seed}.log")
            print(f"{experiment_tags[-1]}:", file=f)
            print(f"\tpython3 ijcnn_exps.py --seed={seed} --n-clients=10 --model={model} --non-iidness={noniid} --tags=IJCNN {test_run_opt} {ds}", file=f)

        exp_tags_str:str = " ".join(experiment_tags)
        print(f"all:{exp_tags_str}", file=f)

        print(f"clean_all_logs:", file=f)
        print(f"\trm -f logs/*.log", file=f)
        print(f"\trm -f logs/*.err", file=f)


if __name__ == '__main__':
    app()
